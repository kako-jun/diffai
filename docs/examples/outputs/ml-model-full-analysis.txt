# diffai Demo Output: ml-model-full-analysis
# Description: Complete ML model analysis (all 30+ features run automatically)
# Command: diffai tests/fixtures/ml_models/simple_base.safetensors tests/fixtures/ml_models/simple_modified.safetensors
# Generated: Mon Jul 14 09:45:23 PM JST 2025
# Version: 0.3.4

## Command Output:

alert_degradation: triggered=false, "All metrics within normal range"
â—¦ analysis_report: type="comprehensive_analysis", findings=2, recommendations=1, confidence=0.92
anomaly_detection: type=dead_neurons, missing_layer, severity=medium, action="monitor_closely_adjust_hyperparameters", regression_test=regression_test_required
architecture_comparison: type1=feedforward, type2=transformer, depth=2â†’15, differences=20, deployment_readiness=not_ready, "thorough_testing_required"
â—¦ attention_analysis: layers=12, pattern_changes=2, focus_shift=0.85, "high"
â—¦ attention_pattern: pattern=stableâ†’minimal_drift, similarity=0.9100, span_change=2, "maintain_current_patterns"
change_summary: layers_changed=17, magnitude=2.0000, patterns=2, most_changed=5
â—¦ clustering_change: clusters=8â†’10, stability=0.8900, migrated=8, new=10, dissolved=9, "slight_increase_in_clusters"
+ convergence_analysis: status=plateaued, stability=1.0000, inference_speed=âš¡ (convergence)
â—¦ deployment_readiness: readiness=0.92, strategy=blue_green, risk=low, timeline=ready_for_immediate_deployment
â—¦ embedding_analysis: dim_change=768â†’768, semantic_drift=0.0300, similarity_preservation=0.9400, clustering_stability=0.8700
â—¦ ensemble_analysis: models=3, diversity=0.72, efficiency=0.88x, redundancy=2 (ensemble)
â—¦ experiment_reproducibility: score=0.91, critical_changes=1, determinism=0.91 (reproducibility)
  + classifier.bias: shape=[10], dtype=f32, params=10
  + classifier.weight: shape=[10, 128], dtype=f32, params=1280
  + embedding.weight: shape=[1000, 128], dtype=f32, params=128000
  - fc.bias: shape=[5], dtype=f32, params=5
  - fc.weight: shape=[5, 10], dtype=f32, params=50
gradient_analysis: flow_health=vanishing, norm=0.000000, ratio=1.0000
â—¦ head_importance: important_heads=2, prunable_heads=2, specializations=4
â—¦ hyperparameter_comparison: changed=1, convergence_impact=0.50, risk=low, "Detected 1 hyperparameter changes. Impact level: low. Monitor convergence carefully." (hyperparameters)
â—¦ hyperparameter_impact: lr_impact=0.1500, batch_impact=0.0800, convergence=0.1200, performance=2.3000
inference_speed: speed_ratio=0.00x, flops_ratio=13132.98x, bottlenecks=0, "optimal_for_deployment"
â—¦ learning_curve_analysis: trend=improving, convergence=None, overfitting_risk=0.23, efficiency=0.78 (learning_curve)
+ learning_progress: trend=improving, magnitude=0.0500, speed=0.80, memory_analysis=ðŸ§  (learning_progress)
â—¦ learning_rate_analysis: current_lr=0.001000, schedule=cosine_decay, effectiveness=0.8700, stability_impact=0.9200, "add_warmup_phase"
memory_analysis: delta=+2.8MB, gpu_est=8.3MB, efficiency=0.000571, review_friendly="optimal_no_action_needed", "optimal_no_action_needed"
â—¦ param_efficiency: efficiency_ratio=0.0001, utilization=0.87, pruning_potential=0.15, category=optimal, bottlenecks=1, "maintain_current_size"
â—¦ performance_impact: latency_change=393959.45%, throughput_change=-262639.64%, memory_change=1313198.18%, category=neutral, confidence=0.8500
â—¦ quantization_analysis: compression=0.0%, speedup=1.8x, precision_loss=1.5%, suitability=good (quantization)
regression_test: passed=true, degradation=-2.5%, severity=low, "proceed_with_deployment"
review_friendly: impact=medium, approval=approve, key_changes=2, "Model improvement with better convergence and performance"
â—¦ risk_assessment: risk=low, readiness=ready, factors=1, rollback=easy
â—¦ similarity_matrix: matrix_dims=15x15, clustering_coeff=0.6000, sparsity=0.4000, outliers=0, metric=cosine
â—¦ statistical_significance: metric=tensor_parameter_differences, p_value=0.2340, effect_size=NaN, power=0.42, "No significant difference detected." (statistical)
â—¦ transfer_learning_analysis: frozen=13/2, updated_params=13.3%, adaptation=moderate, efficiency=0.85 (transfer_learning)
    + transformer.linear1.bias: shape=[2048], dtype=f32, params=2048
    + transformer.linear1.weight: shape=[2048, 128], dtype=f32, params=262144
    + transformer.linear2.bias: shape=[128], dtype=f32, params=128
    + transformer.linear2.weight: shape=[128, 2048], dtype=f32, params=262144
    + transformer.norm1.bias: shape=[128], dtype=f32, params=128
    + transformer.norm1.weight: shape=[128], dtype=f32, params=128
    + transformer.norm2.bias: shape=[128], dtype=f32, params=128
    + transformer.norm2.weight: shape=[128], dtype=f32, params=128
    + transformer.self_attn.in_proj_bias: shape=[384], dtype=f32, params=384
    + transformer.self_attn.in_proj_weight: shape=[384, 128], dtype=f32, params=49152
      + transformer.self_attn.out_proj.bias: shape=[128], dtype=f32, params=128
      + transformer.self_attn.out_proj.weight: shape=[128, 128], dtype=f32, params=16384